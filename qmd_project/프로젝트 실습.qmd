---
title: Chapter6. 프로젝트 실습
engine: knitr
---

Python 프로젝트 실습을 통해 파일 다운로드부터 파이썬 프로젝트 실행까지의 모든 단계를 함께 해보도록 하겠습니다.

여기서는 이미지 캡셔닝(Image Captioning) 프로젝트를 수행합니다.

이미지 캡셔닝이란, 주어진 이미지를 설명하는 문장을 만들어내는 것을 의미합니다. 

예를 들어, 아래와 같은 이미지가 모델에 입력되게 되면, 모델은 “A black dog sitting among leaves in a forest, surrounded by trees.(검은 개가 숲 속 나무들 사이에서 나뭇잎에 둘러싸여 앉아 있는 모습.)”이라는 캡션이 나오게 됩니다.

![](./jpg/dog.jpg)

이러한 캡셔닝 모델을 학습하기 위한 실습을 시작해보도록 하겠습니다.


## 1. 경로 및 환경 설정 {.unnumbered}

프로젝트 수행에 앞서, 항상 주의해야 할 부분은 경로와 환경입니다.

### 경로 설정 {.unnumbered}
먼저 임의의 프로젝트 폴더를 생성하겠습니다. 저는 captioning 이라는 폴더를 생성하였습니다.
Terminal에서도 해당 경로에서 작업한다는 것을 알려주기 위해, cd 명령어를 통해 작업 폴더(captioning 폴더)로 진입하겠습니다.

📌 **참고사항**

명령어 `pwd`는 현재 작업 경로를 확인하는 코드입니다.

명령어 `cd`는 이동하고자하는 작업 경로를 지정하는 코드입니다.


![](./jpg/cd.jpg)

### 환경 설정 {.unnumbered}
다음 프로젝트를 수행하기 위한 가상환경을 생성하겠습니다.

`conda create -n captioning python=3.8` 명령어를 통해 python 버전 3.8을 가진 captioning 이라는 이름의 가상환경을 생성하였습니다.

이제 `conda activate captioning`으로 가상환경에 진입합니다.

![](./jpg/env.jpg)

이후, 아래의 명령어를 통해 프로젝트에 필요한 라이브러리를 다운받겠습니다.

```{bash}
#| eval: false

pip install torch torchvision transformers matplotlib
```

## 2. 데이터셋 다운로드 {.unnumbered}

캡셔닝 모델을 학습하기 위해 사용할 수 있는 데이터셋은 매우 많습니다. 우리는 그 중 **Microsoft COCO (이하 MS COCO)** 데이터셋을 활용해보고자 합니다.

MS COCO는 Object detection(물체 탐지), Segmentation(분류), Captioning에 주로 사용되는 데이터셋으로, 컴퓨터 비전 분야에서 넓은 폭으로 사용되고 있는 데이터셋입니다.

MS COCO 데이터셋 다운받기 위한 shell 스크립트 파일을 작성하겠습니다.

```{bash}
#| eval: false

#!/bin/bash

# COCO dataset directory
mkdir -p /data/coco

# Download COCO Train2014 images and captions
cd /data/coco
wget http://images.cocodataset.org/zips/train2014.zip
wget http://images.cocodataset.org/zips/val2014.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip

# Unzip the dataset
unzip train2014.zip
unzip val2014.zip
unzip annotations_trainval2014.zip
```

✔ `mkdir` 명령어를 통해 data를 저장하고자 하는 경로를 명시해주세요. mkdir는 해당 경로를 생성해주는 명령어입니다.

✔ `cd` 명령어를 통해 생성한 경로로 진입합니다.

✔ `wget` 명령어를 통해 MS COCO dataset을 저장할 수 있는 인터넷 사이트로 접속하여, 파일을 다운로드 받습니다.

✔ `unzip` 명령어를 통해 저장한 dataset의 zip파일을 압축해제하여, 사용할 수 있는 형태로 둡니다.


파일 작성이 완료되었다면, 이제 shell 스크립트 파일을 실행해보도록 하겠습니다.

![](./jpg/shell_error.jpg)

바로 `datset_download.sh` 명령어를 통해 shell 파일을 실행하다보면, "permission denied (권한 오류)"가 발생할 수 있습니다. 파일 실행 권한을 가지기 위해, `chmod` 명령어를 사용합니다.

`chmod` 명령어는 파일의 권한을 바꿔주는 리눅스 명령어로, 명령어 구성은 다음과 같습니다.

```{bash}
#| eval: false

chmod [references][operator][modes] file1 ...
```

| r	| 읽기(read)   |
| w	| 쓰기(write)  |
| x	| 실행(execute)|


우리가 실행한 명령어 `chmod +x [file_name.sh]`는 +x를 통해 실행하는 권한을 [file_name.sh]에 부여한 것입니다.

![](./jpg/shell_error.jpg)

파일 다운로드가 시작되었습니다.

모든 파일이 다운로드 되었다면, 반드시 데이터가 정상적으로 다운로드 되었는지 폴더 내 경로로 진입하여 확인하세요.

![](./jpg/coco_download.jpg)


## 3. 모델 학습 {.unnumbered}

모델을 학습시킬 데이터셋을 다운받았으니, 이제 학습할 모델을 지정해주겠습니다.

여기서는 **Transformer** 모델을 학습시킵니다. Transformer는 2017년 Google에서 발표된 이후로 딥러닝 전역에서 활발하게 사용되고 있는 모델로, 캡셔닝을 학습하기에도 유용한 모델입니다.

아래에서 작성한 transformer.py 파일을 확인할 수 있습니다.
```{python}
#| eval: false
#| python.reticulate: false

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from transformers import ViTModel, BertTokenizer, BertConfig, BertModel

from tqdm import tqdm

# 디바이스 설정 (GPU 사용)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 데이터셋 경로 설정
data_dir = 'data/coco/train2014'  # 이미지가 있는 경로
ann_file = 'data/coco/annotations/captions_train2014.json'  # 주석 파일 경로

# 데이터 로딩을 위한 커스텀 데이터셋 클래스 정의
class CocoDataset(Dataset):
    def __init__(self, data_dir, ann_file, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        with open(ann_file, 'r') as f:
            self.annotations = json.load(f)['annotations']
        self.image_ids = [item['image_id'] for item in self.annotations]
        self.captions = [item['caption'] for item in self.annotations]
    def __len__(self):
        return len(self.annotations)
    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        img_path = os.path.join(self.data_dir, f'COCO_train2014_{image_id:012}.jpg')
        image = Image.open(img_path).convert("RGB")
        caption = self.captions[idx]
        if self.transform:
            image = self.transform(image)
        return image, caption

# 데이터 변환 정의
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# 데이터셋 및 데이터로더 초기화
dataset = CocoDataset(data_dir=data_dir, ann_file=ann_file, transform=transform)
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Transformer 이미지 캡셔닝 모델 정의
class TransformerImageCaptioning(nn.Module):
    def __init__(self, vocab_size):
        super(TransformerImageCaptioning, self).__init__()
        # Vision Transformer for image feature extraction
        self.vit = ViTModel.from_pretrained("google/vit-base-patch16-224-in21k")
        # Transformer Decoder for caption generation
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        config = BertConfig(
            vocab_size=vocab_size,
            num_hidden_layers=6,
            num_attention_heads=12,
            hidden_size=768,
            is_decoder=True,
            add_cross_attention=True
        )
        self.decoder = BertModel(config)
        # Fully connected layer to convert image features to the decoder input size
        self.fc = nn.Linear(self.vit.config.hidden_size, config.hidden_size)
        self.fc_out = nn.Linear(config.hidden_size, vocab_size)
    def forward(self, image, caption_ids):
        # Get image features from ViT
        image_features = self.vit(image).last_hidden_state
        # Average pooling to keep the batch dimension
        image_features = torch.mean(image_features, dim=1)
        image_features = self.fc(image_features)
        # Repeat the image features across the sequence length
        image_features = image_features.unsqueeze(1).repeat(1, caption_ids.size(1), 1)
        # Get text features from decoder (BERT model in decoder mode)
        attention_mask = (caption_ids != self.tokenizer.pad_token_id).float()
        decoder_outputs = self.decoder(input_ids=caption_ids, attention_mask=attention_mask, encoder_hidden_states=image_features)
        logits = self.fc_out(decoder_outputs.last_hidden_state)
        return logits

# 모델 및 손실 함수, 옵티마이저 설정
vocab_size = 30522  # BERT 토크나이저의 기본 vocab_size
model = TransformerImageCaptioning(vocab_size).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)
optimizer = optim.Adam(model.parameters(), lr=5e-5)


# 모델 학습
def train_model(data_loader, model, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        print(f"Starting epoch {epoch + 1}/{num_epochs}")
        epoch_loss = 0
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1}")
        for i, (images, captions) in enumerate(progress_bar):
            images = images.to(device)
            # Tokenize captions
            caption_ids = model.tokenizer(captions, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)
            optimizer.zero_grad()
            outputs = model(images, caption_ids)
            # Align dimensions
            outputs = outputs.view(-1, vocab_size)
            caption_ids = caption_ids.view(-1)
            loss = criterion(outputs, caption_ids)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(data_loader):.4f}')
    print("Training completed")

train_model(data_loader, model, criterion, optimizer, num_epochs=5)

# 모델 저장
model_save_path = './model/transformer_image_captioning_model.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Model saved at {model_save_path}")

```

이 파이썬 프로젝트는 slurm을 통해 실행합니다. 이를 위해, HPC 환경에서 모델을 학습하기 위한 slurm 스크립트 `transformer.sh`를 작성합니다.

```{bash}
#| eval: false

#!/bin/bash
#SBATCH --job-name=captioning
#SBATCH --output=./output/training_captioning_%n_%j.out
#SBATCH --error=./output/training_captioning_%n_%j.err
#SBATCH --nodes=2
#SBATCH --partition=gpu3
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=16
#SBATCH --mem=128G
#SBATCH --time=24:00:00

echo "start at:" `date` # 접속한 날짜 표기
echo "node: $HOSTNAME" # 접속한 노드 번호 표기
echo "jobid: $SLURM_JOB_ID" # jobid 표기

# Load modules
module load cuda/11.8

# Train the transformer-based image captioning model
python transformer.py
```

✔ **`#SBATCH --job-name=captioning`**
job-name을 captioning으로 지정하였습니다.

✔ output 파일과 error 파일은 output 폴더의 training_captioning이라는 파일명으로 지정하였습니다.

✔ **`#SBATCH --nodes=2`**
복잡한 작업을 요구하는 프로젝트인 만큼, 시간의 효율성을 위해 node 2개를 선택하였습니다. 이에 따라, node 2개로 하나의 작업을 수행하는 병렬 컴퓨팅을 시행합니다.

✔ **`#SBATCH --gres=gpu:4`**
gpu는 4대를 사용하였습니다.

✔ **`module load cuda/11.8`**
module은 cuda 11.8 version을 사용하였습니다.

✔ **`python transformer.py`**
`transformer.py` 파일을 실행합니다.

`sbatch transformer.sh`를 통해 작업(job)을 할당하였습니다.

프로젝트에서 tqdm 라이브러리를 통해 진척도를 확인할 수 있게 세팅해둠에 따라, error 파일에서 진척도를 시각적으로 확인할 수 있습니다.

![](./jpg/epoch.jpg)

out 파일에서 학습된 log를 확인해보면, epoch이 진행될수록 손실 값(loss)가 꾸준히 감소하고 있는 것을 볼 수 있습니다. 학습이 잘 된 듯 합니다. 모델 또한 잘 저장되어있음을 탐색기에서 확인하였습니다.

```{bash}
#| eval: false

start at: Mon Oct 28 14:07:42 KST 2024
node: n063
jobid: 247333
Starting epoch 1/5
Epoch [1/5], Loss: 0.1303
Starting epoch 2/5
Epoch [2/5], Loss: 0.0072
Starting epoch 3/5
Epoch [3/5], Loss: 0.0025
Starting epoch 4/5
Epoch [4/5], Loss: 0.0006
Starting epoch 5/5
Epoch [5/5], Loss: 0.0001
Training completed
Model saved at ./model/transformer_image_captioning_model.pth

```

## 4. 모델 검증 {.unnumbered}

실제로 모델이 학습이 잘 되었는지 검증해보도록 하겠습니다. 검증 데이터셋을 활용하여 캡셔닝 성능을 평가할 수 있습니다. 이를 위해 다음과 같은 과정이 필요합니다.

(1) 검증 데이터셋을 준비하여, 로드합니다. 여기서는 coco dataset의 val2014 dataset을 활용합니다.

(2) 모델이 검증 데이터셋에서 예측을 수행하고, 예측 결과를 실제 값과 비교하는 함수를 작성합니다.

(3) 마지막으로 예측 결과에 대한 평가 지표를 계산합니다.

다음과 같은 코드를 작성하여, 모델의 성능을 평가해보겠습니다.

```{python}
#| eval: false
#| python.reticulate: false

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from transformers import ViTModel, BertTokenizer, BertConfig, BertModel
from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
# 모델 로드 함수
def load_model(model_path, vocab_size):
    model = TransformerImageCaptioning(vocab_size).to(device)
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model.eval()
    return model

# 검증 데이터 로드 함수
def load_validation_data(data_dir, ann_file, transform):
    class CocoDataset(Dataset):
        def __init__(self, data_dir, ann_file, transform=None):
            self.data_dir = data_dir
            self.transform = transform
            with open(ann_file, 'r') as f:
                self.annotations = json.load(f)['annotations']
            self.image_ids = [item['image_id'] for item in self.annotations]
            self.captions = [item['caption'] for item in self.annotations]
        def __len__(self):
            return len(self.annotations)
        def __getitem__(self, idx):
            image_id = self.image_ids[idx]
            img_path = os.path.join(self.data_dir, f'COCO_val2014_{image_id:012}.jpg')
            image = Image.open(img_path).convert("RGB")
            caption = self.captions[idx]
            if self.transform:
                image = self.transform(image)
            return image, caption, image_id

    dataset = CocoDataset(data_dir, ann_file, transform)
    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)
    return data_loader

# 캡셔닝 검증 함수
def validate_model(model, data_loader):
    model.eval()
    tokenizer = model.tokenizer
    total_loss = 0
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    with torch.no_grad():
        for images, captions, image_ids in tqdm(data_loader, desc="Validating"):
            images = images.to(device)
            caption_ids = tokenizer(captions, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)
            outputs = model(images, caption_ids)
            outputs = outputs.view(-1, tokenizer.vocab_size)
            caption_ids = caption_ids.view(-1)
            loss = criterion(outputs, caption_ids)
            total_loss += loss.item()
    avg_loss = total_loss / len(data_loader)
    print(f'Validation Loss: {avg_loss:.4f}')

# 검증 데이터셋 로드
data_dir = 'data/coco/val2014'
ann_file = 'data/coco/annotations/captions_val2014.json'
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
validation_loader = load_validation_data(data_dir, ann_file, transform)

# 검증 수행
vocab_size = 30522
model_path = './model/transformer_image_captioning_model.pth'
model = load_model(model_path, vocab_size)
validate_model(model, validation_loader)

```

동일하게 shell 스크립트를 작성하여 `sbatch val.py` 명령을 통해 작업(job)을 수행하였습니다.
out 파일을 확인한 결과, 다음과 같이 손실(Loss)값 0.0073으로 높은 성능을 보이는 것을 확인할 수 있습니다.

```{bash}
#| eval: false

start at: Tue Oct 29 11:13:22 KST 2024
node: n083
jobid: 247856
Validation Loss: 0.0073

```

