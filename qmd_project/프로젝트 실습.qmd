---
title: Chapter6. í”„ë¡œì íŠ¸ ì‹¤ìŠµ
engine: knitr
---

Python í”„ë¡œì íŠ¸ ì‹¤ìŠµì„ í†µí•´ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¶€í„° íŒŒì´ì¬ í”„ë¡œì íŠ¸ ì‹¤í–‰ê¹Œì§€ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ í•¨ê»˜ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” ì´ë¯¸ì§€ ìº¡ì…”ë‹(Image Captioning) í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì´ë¯¸ì§€ ìº¡ì…”ë‹ì´ë€, ì£¼ì–´ì§„ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì´ë¯¸ì§€ê°€ ëª¨ë¸ì— ì…ë ¥ë˜ê²Œ ë˜ë©´, ëª¨ë¸ì€ â€œA black dog sitting among leaves in a forest, surrounded by trees.(ê²€ì€ ê°œê°€ ìˆ² ì† ë‚˜ë¬´ë“¤ ì‚¬ì´ì—ì„œ ë‚˜ë­‡ìì— ë‘˜ëŸ¬ì‹¸ì—¬ ì•‰ì•„ ìˆëŠ” ëª¨ìŠµ.)â€ì´ë¼ëŠ” ìº¡ì…˜ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.

![](./jpg/dog.jpg)

ì´ëŸ¬í•œ ìº¡ì…”ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹¤ìŠµì„ ì‹œì‘í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.


## 1. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì • {.unnumbered}

í”„ë¡œì íŠ¸ ìˆ˜í–‰ì— ì•ì„œ, í•­ìƒ ì£¼ì˜í•´ì•¼ í•  ë¶€ë¶„ì€ ê²½ë¡œì™€ í™˜ê²½ì…ë‹ˆë‹¤.

### ê²½ë¡œ ì„¤ì • {.unnumbered}
ë¨¼ì € ì„ì˜ì˜ í”„ë¡œì íŠ¸ í´ë”ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤. ì €ëŠ” captioning ì´ë¼ëŠ” í´ë”ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
Terminalì—ì„œë„ í•´ë‹¹ ê²½ë¡œì—ì„œ ì‘ì—…í•œë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê¸° ìœ„í•´, cd ëª…ë ¹ì–´ë¥¼ í†µí•´ ì‘ì—… í´ë”(captioning í´ë”)ë¡œ ì§„ì…í•˜ê² ìŠµë‹ˆë‹¤.

ğŸ“Œ **ì°¸ê³ ì‚¬í•­**

ëª…ë ¹ì–´ `pwd`ëŠ” í˜„ì¬ ì‘ì—… ê²½ë¡œë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

ëª…ë ¹ì–´ `cd`ëŠ” ì´ë™í•˜ê³ ìí•˜ëŠ” ì‘ì—… ê²½ë¡œë¥¼ ì§€ì •í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.


![](./jpg/cd.jpg)

### í™˜ê²½ ì„¤ì • {.unnumbered}
ë‹¤ìŒ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.

`conda create -n captioning python=3.8` ëª…ë ¹ì–´ë¥¼ í†µí•´ python ë²„ì „ 3.8ì„ ê°€ì§„ captioning ì´ë¼ëŠ” ì´ë¦„ì˜ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

ì´ì œ `conda activate captioning`ìœ¼ë¡œ ê°€ìƒí™˜ê²½ì— ì§„ì…í•©ë‹ˆë‹¤.

![](./jpg/env.jpg)

ì´í›„, ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë‹¤ìš´ë°›ê² ìŠµë‹ˆë‹¤.

```{bash}
#| eval: false

pip install torch torchvision transformers matplotlib
```

## 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ {.unnumbered}

ìº¡ì…”ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¸ ì¤‘ **Microsoft COCO (ì´í•˜ MS COCO)** ë°ì´í„°ì…‹ì„ í™œìš©í•´ë³´ê³ ì í•©ë‹ˆë‹¤.

MS COCOëŠ” Object detection(ë¬¼ì²´ íƒì§€), Segmentation(ë¶„ë¥˜), Captioningì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë„“ì€ í­ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

MS COCO ë°ì´í„°ì…‹ ë‹¤ìš´ë°›ê¸° ìœ„í•œ shell ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

```{bash}
#| eval: false

#!/bin/bash

# COCO dataset directory
mkdir -p /data/coco

# Download COCO Train2014 images and captions
cd /data/coco
wget http://images.cocodataset.org/zips/train2014.zip
wget http://images.cocodataset.org/zips/val2014.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip

# Unzip the dataset
unzip train2014.zip
unzip val2014.zip
unzip annotations_trainval2014.zip
```

âœ” `mkdir` ëª…ë ¹ì–´ë¥¼ í†µí•´ dataë¥¼ ì €ì¥í•˜ê³ ì í•˜ëŠ” ê²½ë¡œë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”. mkdirëŠ” í•´ë‹¹ ê²½ë¡œë¥¼ ìƒì„±í•´ì£¼ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.

âœ” `cd` ëª…ë ¹ì–´ë¥¼ í†µí•´ ìƒì„±í•œ ê²½ë¡œë¡œ ì§„ì…í•©ë‹ˆë‹¤.

âœ” `wget` ëª…ë ¹ì–´ë¥¼ í†µí•´ MS COCO datasetì„ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì¸í„°ë„· ì‚¬ì´íŠ¸ë¡œ ì ‘ì†í•˜ì—¬, íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.

âœ” `unzip` ëª…ë ¹ì–´ë¥¼ í†µí•´ ì €ì¥í•œ datasetì˜ zipíŒŒì¼ì„ ì••ì¶•í•´ì œí•˜ì—¬, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë‘¡ë‹ˆë‹¤.


íŒŒì¼ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ shell ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‹¤í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

![](./jpg/shell_error.jpg)

ë°”ë¡œ `datset_download.sh` ëª…ë ¹ì–´ë¥¼ í†µí•´ shell íŒŒì¼ì„ ì‹¤í–‰í•˜ë‹¤ë³´ë©´, "permission denied (ê¶Œí•œ ì˜¤ë¥˜)"ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ì‹¤í–‰ ê¶Œí•œì„ ê°€ì§€ê¸° ìœ„í•´, `chmod` ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

`chmod` ëª…ë ¹ì–´ëŠ” íŒŒì¼ì˜ ê¶Œí•œì„ ë°”ê¿”ì£¼ëŠ” ë¦¬ëˆ…ìŠ¤ ëª…ë ¹ì–´ë¡œ, ëª…ë ¹ì–´ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```{bash}
#| eval: false

chmod [references][operator][modes] file1 ...
```

| r	| ì½ê¸°(read)   |
| w	| ì“°ê¸°(write)  |
| x	| ì‹¤í–‰(execute)|


ìš°ë¦¬ê°€ ì‹¤í–‰í•œ ëª…ë ¹ì–´ `chmod +x [file_name.sh]`ëŠ” +xë¥¼ í†µí•´ ì‹¤í–‰í•˜ëŠ” ê¶Œí•œì„ [file_name.sh]ì— ë¶€ì—¬í•œ ê²ƒì…ë‹ˆë‹¤.

![](./jpg/shell_error.jpg)

íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

ëª¨ë“  íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆë‹¤ë©´, ë°˜ë“œì‹œ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í´ë” ë‚´ ê²½ë¡œë¡œ ì§„ì…í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.

![](./jpg/coco_download.jpg)


## 3. ëª¨ë¸ í•™ìŠµ {.unnumbered}

ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë°›ì•˜ìœ¼ë‹ˆ, ì´ì œ í•™ìŠµí•  ëª¨ë¸ì„ ì§€ì •í•´ì£¼ê² ìŠµë‹ˆë‹¤.

ì—¬ê¸°ì„œëŠ” **Transformer** ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. TransformerëŠ” 2017ë…„ Googleì—ì„œ ë°œí‘œëœ ì´í›„ë¡œ ë”¥ëŸ¬ë‹ ì „ì—­ì—ì„œ í™œë°œí•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëª¨ë¸ë¡œ, ìº¡ì…”ë‹ì„ í•™ìŠµí•˜ê¸°ì—ë„ ìœ ìš©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

ì•„ë˜ì—ì„œ ì‘ì„±í•œ transformer.py íŒŒì¼ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```{python}
#| eval: false
#| python.reticulate: false

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from transformers import ViTModel, BertTokenizer, BertConfig, BertModel

from tqdm import tqdm

# ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš©)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
data_dir = 'data/coco/train2014'  # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ë¡œ
ann_file = 'data/coco/annotations/captions_train2014.json'  # ì£¼ì„ íŒŒì¼ ê²½ë¡œ

# ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class CocoDataset(Dataset):
    def __init__(self, data_dir, ann_file, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        with open(ann_file, 'r') as f:
            self.annotations = json.load(f)['annotations']
        self.image_ids = [item['image_id'] for item in self.annotations]
        self.captions = [item['caption'] for item in self.annotations]
    def __len__(self):
        return len(self.annotations)
    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        img_path = os.path.join(self.data_dir, f'COCO_train2014_{image_id:012}.jpg')
        image = Image.open(img_path).convert("RGB")
        caption = self.captions[idx]
        if self.transform:
            image = self.transform(image)
        return image, caption

# ë°ì´í„° ë³€í™˜ ì •ì˜
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì´ˆê¸°í™”
dataset = CocoDataset(data_dir=data_dir, ann_file=ann_file, transform=transform)
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Transformer ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ ì •ì˜
class TransformerImageCaptioning(nn.Module):
    def __init__(self, vocab_size):
        super(TransformerImageCaptioning, self).__init__()
        # Vision Transformer for image feature extraction
        self.vit = ViTModel.from_pretrained("google/vit-base-patch16-224-in21k")
        # Transformer Decoder for caption generation
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        config = BertConfig(
            vocab_size=vocab_size,
            num_hidden_layers=6,
            num_attention_heads=12,
            hidden_size=768,
            is_decoder=True,
            add_cross_attention=True
        )
        self.decoder = BertModel(config)
        # Fully connected layer to convert image features to the decoder input size
        self.fc = nn.Linear(self.vit.config.hidden_size, config.hidden_size)
        self.fc_out = nn.Linear(config.hidden_size, vocab_size)
    def forward(self, image, caption_ids):
        # Get image features from ViT
        image_features = self.vit(image).last_hidden_state
        # Average pooling to keep the batch dimension
        image_features = torch.mean(image_features, dim=1)
        image_features = self.fc(image_features)
        # Repeat the image features across the sequence length
        image_features = image_features.unsqueeze(1).repeat(1, caption_ids.size(1), 1)
        # Get text features from decoder (BERT model in decoder mode)
        attention_mask = (caption_ids != self.tokenizer.pad_token_id).float()
        decoder_outputs = self.decoder(input_ids=caption_ids, attention_mask=attention_mask, encoder_hidden_states=image_features)
        logits = self.fc_out(decoder_outputs.last_hidden_state)
        return logits

# ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •
vocab_size = 30522  # BERT í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ vocab_size
model = TransformerImageCaptioning(vocab_size).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)
optimizer = optim.Adam(model.parameters(), lr=5e-5)


# ëª¨ë¸ í•™ìŠµ
def train_model(data_loader, model, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        print(f"Starting epoch {epoch + 1}/{num_epochs}")
        epoch_loss = 0
        progress_bar = tqdm(data_loader, desc=f"Epoch {epoch + 1}")
        for i, (images, captions) in enumerate(progress_bar):
            images = images.to(device)
            # Tokenize captions
            caption_ids = model.tokenizer(captions, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)
            optimizer.zero_grad()
            outputs = model(images, caption_ids)
            # Align dimensions
            outputs = outputs.view(-1, vocab_size)
            caption_ids = caption_ids.view(-1)
            loss = criterion(outputs, caption_ids)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(data_loader):.4f}')
    print("Training completed")

train_model(data_loader, model, criterion, optimizer, num_epochs=5)

# ëª¨ë¸ ì €ì¥
model_save_path = './model/transformer_image_captioning_model.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Model saved at {model_save_path}")

```

ì´ íŒŒì´ì¬ í”„ë¡œì íŠ¸ëŠ” slurmì„ í†µí•´ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, HPC í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ slurm ìŠ¤í¬ë¦½íŠ¸ `transformer.sh`ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

```{bash}
#| eval: false

#!/bin/bash
#SBATCH --job-name=captioning
#SBATCH --output=./output/training_captioning_%n_%j.out
#SBATCH --error=./output/training_captioning_%n_%j.err
#SBATCH --nodes=2
#SBATCH --partition=gpu3
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=16
#SBATCH --mem=128G
#SBATCH --time=24:00:00

echo "start at:" `date` # ì ‘ì†í•œ ë‚ ì§œ í‘œê¸°
echo "node: $HOSTNAME" # ì ‘ì†í•œ ë…¸ë“œ ë²ˆí˜¸ í‘œê¸°
echo "jobid: $SLURM_JOB_ID" # jobid í‘œê¸°

# Load modules
module load cuda/11.8

# Train the transformer-based image captioning model
python transformer.py
```

âœ” **`#SBATCH --job-name=captioning`**
job-nameì„ captioningìœ¼ë¡œ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

âœ” output íŒŒì¼ê³¼ error íŒŒì¼ì€ output í´ë”ì˜ training_captioningì´ë¼ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

âœ” **`#SBATCH --nodes=2`**
ë³µì¡í•œ ì‘ì—…ì„ ìš”êµ¬í•˜ëŠ” í”„ë¡œì íŠ¸ì¸ ë§Œí¼, ì‹œê°„ì˜ íš¨ìœ¨ì„±ì„ ìœ„í•´ node 2ê°œë¥¼ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼, node 2ê°œë¡œ í•˜ë‚˜ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë³‘ë ¬ ì»´í“¨íŒ…ì„ ì‹œí–‰í•©ë‹ˆë‹¤.

âœ” **`#SBATCH --gres=gpu:4`**
gpuëŠ” 4ëŒ€ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

âœ” **`module load cuda/11.8`**
moduleì€ cuda 11.8 versionì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

âœ” **`python transformer.py`**
`transformer.py` íŒŒì¼ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

`sbatch transformer.sh`ë¥¼ í†µí•´ ì‘ì—…(job)ì„ í• ë‹¹í•˜ì˜€ìŠµë‹ˆë‹¤.

í”„ë¡œì íŠ¸ì—ì„œ tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì§„ì²™ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ ì„¸íŒ…í•´ë‘ ì— ë”°ë¼, error íŒŒì¼ì—ì„œ ì§„ì²™ë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![](./jpg/epoch.jpg)

out íŒŒì¼ì—ì„œ í•™ìŠµëœ logë¥¼ í™•ì¸í•´ë³´ë©´, epochì´ ì§„í–‰ë ìˆ˜ë¡ ì†ì‹¤ ê°’(loss)ê°€ ê¾¸ì¤€íˆ ê°ì†Œí•˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµì´ ì˜ ëœ ë“¯ í•©ë‹ˆë‹¤. ëª¨ë¸ ë˜í•œ ì˜ ì €ì¥ë˜ì–´ìˆìŒì„ íƒìƒ‰ê¸°ì—ì„œ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

```{bash}
#| eval: false

start at: Mon Oct 28 14:07:42 KST 2024
node: n063
jobid: 247333
Starting epoch 1/5
Epoch [1/5], Loss: 0.1303
Starting epoch 2/5
Epoch [2/5], Loss: 0.0072
Starting epoch 3/5
Epoch [3/5], Loss: 0.0025
Starting epoch 4/5
Epoch [4/5], Loss: 0.0006
Starting epoch 5/5
Epoch [5/5], Loss: 0.0001
Training completed
Model saved at ./model/transformer_image_captioning_model.pth

```

## 4. ëª¨ë¸ ê²€ì¦ {.unnumbered}

ì‹¤ì œë¡œ ëª¨ë¸ì´ í•™ìŠµì´ ì˜ ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê²€ì¦ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ìº¡ì…”ë‹ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

(1) ê²€ì¦ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ì—¬, ë¡œë“œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” coco datasetì˜ val2014 datasetì„ í™œìš©í•©ë‹ˆë‹¤.

(2) ëª¨ë¸ì´ ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹¤ì œ ê°’ê³¼ ë¹„êµí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

(3) ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.

```{python}
#| eval: false
#| python.reticulate: false

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from transformers import ViTModel, BertTokenizer, BertConfig, BertModel
from tqdm import tqdm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_model(model_path, vocab_size):
    model = TransformerImageCaptioning(vocab_size).to(device)
    state_dict = torch.load(model_path, map_location=device)
    model.load_state_dict(state_dict)
    model.eval()
    return model

# ê²€ì¦ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_validation_data(data_dir, ann_file, transform):
    class CocoDataset(Dataset):
        def __init__(self, data_dir, ann_file, transform=None):
            self.data_dir = data_dir
            self.transform = transform
            with open(ann_file, 'r') as f:
                self.annotations = json.load(f)['annotations']
            self.image_ids = [item['image_id'] for item in self.annotations]
            self.captions = [item['caption'] for item in self.annotations]
        def __len__(self):
            return len(self.annotations)
        def __getitem__(self, idx):
            image_id = self.image_ids[idx]
            img_path = os.path.join(self.data_dir, f'COCO_val2014_{image_id:012}.jpg')
            image = Image.open(img_path).convert("RGB")
            caption = self.captions[idx]
            if self.transform:
                image = self.transform(image)
            return image, caption, image_id

    dataset = CocoDataset(data_dir, ann_file, transform)
    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)
    return data_loader

# ìº¡ì…”ë‹ ê²€ì¦ í•¨ìˆ˜
def validate_model(model, data_loader):
    model.eval()
    tokenizer = model.tokenizer
    total_loss = 0
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    with torch.no_grad():
        for images, captions, image_ids in tqdm(data_loader, desc="Validating"):
            images = images.to(device)
            caption_ids = tokenizer(captions, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)
            outputs = model(images, caption_ids)
            outputs = outputs.view(-1, tokenizer.vocab_size)
            caption_ids = caption_ids.view(-1)
            loss = criterion(outputs, caption_ids)
            total_loss += loss.item()
    avg_loss = total_loss / len(data_loader)
    print(f'Validation Loss: {avg_loss:.4f}')

# ê²€ì¦ ë°ì´í„°ì…‹ ë¡œë“œ
data_dir = 'data/coco/val2014'
ann_file = 'data/coco/annotations/captions_val2014.json'
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
validation_loader = load_validation_data(data_dir, ann_file, transform)

# ê²€ì¦ ìˆ˜í–‰
vocab_size = 30522
model_path = './model/transformer_image_captioning_model.pth'
model = load_model(model_path, vocab_size)
validate_model(model, validation_loader)

```

ë™ì¼í•˜ê²Œ shell ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ì—¬ `sbatch val.py` ëª…ë ¹ì„ í†µí•´ ì‘ì—…(job)ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
out íŒŒì¼ì„ í™•ì¸í•œ ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì´ ì†ì‹¤(Loss)ê°’ 0.0073ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```{bash}
#| eval: false

start at: Tue Oct 29 11:13:22 KST 2024
node: n083
jobid: 247856
Validation Loss: 0.0073

```

